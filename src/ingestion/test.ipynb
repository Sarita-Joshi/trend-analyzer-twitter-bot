{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2da7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "\n",
    "cnn_paper = newspaper.build('http://cnn.com')\n",
    "for article in cnn_paper.articles:\n",
    "    print(article.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26a25994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "url = 'https://news.google.com/rss/articles/CBMibkFVX3lxTE5LcTlDY05RZE9ycDRaQ2xZamdSdVpidThFclBQVXFtUXFiMXNyVUpzUVNMZjEyZm83UjgwX2w5NldJVlItdzVmenZTY2dGeUZRUUNZc2N4N2xYZDNrWFlmbVlMMzNkODl0MERzZjBB?oc=5'\n",
    "url = 'https://www.nytimes.com/2025/06/19/science/math-ai-darpa.html'\n",
    "url = 'https://news.google.com/rss/articles/CBMif0FVX3lxTFBkOVZURUF2c0dGOXFXV1JKNzVxbTBwdFZCTmthQkY3VnczX2s1M0dfTTJ6Q21PVlB1cEJNcDRoaWphNUlDYjhQczBuZXloYVFTYnVJcWgyVkNWM3I5LVVvY0NNZ184VHVyd0t3d1p3T0g1SzZfamc5XzZSSGhnWnM?oc=5'\n",
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fead3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_link = 'https://news.google.com/rss/articles/CBMif0FVX3lxTFBkOVZURUF2c0dGOXFXV1JKNzVxbTBwdFZCTmthQkY3VnczX2s1M0dfTTJ6Q21PVlB1cEJNcDRoaWphNUlDYjhQczBuZXloYVFTYnVJcWgyVkNWM3I5LVVvY0NNZ184VHVyd0t3d1p3T0g1SzZfamc5XzZSSGhnWnM?oc=5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c0312da",
   "metadata": {},
   "outputs": [],
   "source": [
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fda2492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d26a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import feedparser\n",
    "import requests\n",
    "\n",
    "def resolve_redirect(google_news_url):\n",
    "    try:\n",
    "        response = requests.get(google_news_url, timeout=10, allow_redirects=True)\n",
    "        return response.url\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Redirect resolution failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_news_from_feed(feed_url):\n",
    "    articles = []\n",
    "    feed = feedparser.parse(resolve_redirect(feed_url))\n",
    "    for entry in feed.entries:\n",
    "        return entry\n",
    "        # create a newspaper article object\n",
    "        article = newspaper.Article(entry.link)\n",
    "        # download and parse the article\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        # extract relevant information\n",
    "        articles.append({\n",
    "            'title': article.title,\n",
    "            'author': article.authors,\n",
    "            'publish_date': article.publish_date,\n",
    "            'content': article.text\n",
    "        })\n",
    "        break\n",
    "    return articles\n",
    "\n",
    "\n",
    "# feed_url = 'https://news.google.com/rss/search?q=AI'\n",
    "# articles = scrape_news_from_feed(feed_url)\n",
    "\n",
    "# # print the extracted articles\n",
    "# for article in articles:\n",
    "#     print('Title:', article['title'])\n",
    "#     print('Author:', article['author'])\n",
    "#     print('Publish Date:', article['publish_date'])\n",
    "#     print('Content:', article['content'])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "004302cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = scrape_news_from_feed(feed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43803183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Can A.I. Quicken the Pace of Math Discoveries? - The New York Times',\n",
       " 'title_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': 'https://news.google.com/rss/search?q=AI&hl=en-US&gl=US&ceid=US:en',\n",
       "  'value': 'Can A.I. Quicken the Pace of Math Discoveries? - The New York Times'},\n",
       " 'links': [{'rel': 'alternate',\n",
       "   'type': 'text/html',\n",
       "   'href': 'https://news.google.com/rss/articles/CBMibkFVX3lxTE5LcTlDY05RZE9ycDRaQ2xZamdSdVpidThFclBQVXFtUXFiMXNyVUpzUVNMZjEyZm83UjgwX2w5NldJVlItdzVmenZTY2dGeUZRUUNZc2N4N2xYZDNrWFlmbVlMMzNkODl0MERzZjBB?oc=5'}],\n",
       " 'link': 'https://news.google.com/rss/articles/CBMibkFVX3lxTE5LcTlDY05RZE9ycDRaQ2xZamdSdVpidThFclBQVXFtUXFiMXNyVUpzUVNMZjEyZm83UjgwX2w5NldJVlItdzVmenZTY2dGeUZRUUNZc2N4N2xYZDNrWFlmbVlMMzNkODl0MERzZjBB?oc=5',\n",
       " 'id': 'CBMibkFVX3lxTE5LcTlDY05RZE9ycDRaQ2xZamdSdVpidThFclBQVXFtUXFiMXNyVUpzUVNMZjEyZm83UjgwX2w5NldJVlItdzVmenZTY2dGeUZRUUNZc2N4N2xYZDNrWFlmbVlMMzNkODl0MERzZjBB',\n",
       " 'guidislink': False,\n",
       " 'published': 'Thu, 19 Jun 2025 09:02:06 GMT',\n",
       " 'published_parsed': time.struct_time(tm_year=2025, tm_mon=6, tm_mday=19, tm_hour=9, tm_min=2, tm_sec=6, tm_wday=3, tm_yday=170, tm_isdst=0),\n",
       " 'summary': '<a href=\"https://news.google.com/rss/articles/CBMibkFVX3lxTE5LcTlDY05RZE9ycDRaQ2xZamdSdVpidThFclBQVXFtUXFiMXNyVUpzUVNMZjEyZm83UjgwX2w5NldJVlItdzVmenZTY2dGeUZRUUNZc2N4N2xYZDNrWFlmbVlMMzNkODl0MERzZjBB?oc=5\" target=\"_blank\">Can A.I. Quicken the Pace of Math Discoveries?</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The New York Times</font>',\n",
       " 'summary_detail': {'type': 'text/html',\n",
       "  'language': None,\n",
       "  'base': 'https://news.google.com/rss/search?q=AI&hl=en-US&gl=US&ceid=US:en',\n",
       "  'value': '<a href=\"https://news.google.com/rss/articles/CBMibkFVX3lxTE5LcTlDY05RZE9ycDRaQ2xZamdSdVpidThFclBQVXFtUXFiMXNyVUpzUVNMZjEyZm83UjgwX2w5NldJVlItdzVmenZTY2dGeUZRUUNZc2N4N2xYZDNrWFlmbVlMMzNkODl0MERzZjBB?oc=5\" target=\"_blank\">Can A.I. Quicken the Pace of Math Discoveries?</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">The New York Times</font>'},\n",
       " 'source': {'href': 'https://www.nytimes.com', 'title': 'The New York Times'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "399b4b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(feed_url, timeout=10, allow_redirects=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f61d155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.is_permanent_redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85fc05dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-news-feed\n",
      "  Obtaining dependency information for google-news-feed from https://files.pythonhosted.org/packages/c5/fc/4780664bcfaf258a89357947e86cbe8c261c247be40463a248412c273e45/google_news_feed-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading google_news_feed-1.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting aiohttp>=3.8.4 (from google-news-feed)\n",
      "  Obtaining dependency information for aiohttp>=3.8.4 from https://files.pythonhosted.org/packages/f8/b6/98518bcc615ef998a64bef371178b9afc98ee25895b4f476c428fade2220/aiohttp-3.12.13-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.12.13-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.2 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from google-news-feed) (4.13.4)\n",
      "Collecting dateparser>=1.1.1 (from google-news-feed)\n",
      "  Obtaining dependency information for dateparser>=1.1.1 from https://files.pythonhosted.org/packages/cf/0a/981c438c4cd84147c781e4e96c1d72df03775deb1bc76c5a6ee8afa89c62/dateparser-1.2.1-py3-none-any.whl.metadata\n",
      "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: lxml>=4.9.0 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from google-news-feed) (5.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from google-news-feed) (2.32.4)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.8.4->google-news-feed)\n",
      "  Obtaining dependency information for aiohappyeyeballs>=2.5.0 from https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp>=3.8.4->google-news-feed)\n",
      "  Obtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from aiohttp>=3.8.4->google-news-feed) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.8.4->google-news-feed)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/58/17/fe61124c5c333ae87f09bb67186d65038834a47d974fc10a5fadb4cc5ae1/frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.8.4->google-news-feed)\n",
      "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/e1/fe/bbd85ae65c96de5c9910c332ee1f4b7be0bf0fb21563895167bcb6502a1f/multidict-6.5.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading multidict-6.5.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.8.4->google-news-feed)\n",
      "  Obtaining dependency information for propcache>=0.2.0 from https://files.pythonhosted.org/packages/e1/2d/89fe4489a884bc0da0c3278c552bd4ffe06a1ace559db5ef02ef24ab446b/propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.8.4->google-news-feed)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.17.0 from https://files.pythonhosted.org/packages/f0/09/d9c7942f8f05c32ec72cd5c8e041c8b29b5807328b68b4801ff2511d4d5e/yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "     ---------------------------------------- 0.0/76.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 76.3/76.3 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from beautifulsoup4>=4.12.2->google-news-feed) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from beautifulsoup4>=4.12.2->google-news-feed) (4.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from dateparser>=1.1.1->google-news-feed) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from dateparser>=1.1.1->google-news-feed) (2025.2)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from dateparser>=1.1.1->google-news-feed) (2024.11.6)\n",
      "Collecting tzlocal>=0.2 (from dateparser>=1.1.1->google-news-feed)\n",
      "  Obtaining dependency information for tzlocal>=0.2 from https://files.pythonhosted.org/packages/c2/14/e2a54fabd4f08cd7af1c07030603c3356b74da07f7cc056e600436edfa17/tzlocal-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests>=2.31.0->google-news-feed) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests>=2.31.0->google-news-feed) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests>=2.31.0->google-news-feed) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests>=2.31.0->google-news-feed) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from python-dateutil>=2.7.0->dateparser>=1.1.1->google-news-feed) (1.17.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from tzlocal>=0.2->dateparser>=1.1.1->google-news-feed) (2025.2)\n",
      "Downloading google_news_feed-1.1.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading aiohttp-3.12.13-cp311-cp311-win_amd64.whl (451 kB)\n",
      "   ---------------------------------------- 0.0/451.4 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 235.5/451.4 kB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 451.4/451.4 kB 7.1 MB/s eta 0:00:00\n",
      "Downloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
      "   ---------------------------------------- 0.0/295.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 295.7/295.7 kB 8.9 MB/s eta 0:00:00\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.0/44.0 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading multidict-6.5.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.4/44.4 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.5/41.5 kB ? eta 0:00:00\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.7/86.7 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tzlocal, propcache, multidict, frozenlist, aiohappyeyeballs, yarl, dateparser, aiosignal, aiohttp, google-news-feed\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 dateparser-1.2.1 frozenlist-1.7.0 google-news-feed-1.1.0 multidict-6.5.0 propcache-0.3.2 tzlocal-5.3.1 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install google-news-feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12e2f93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.google.com/rss/articles/CBMi0AFBVV95cUxPaXhYSDhNMXExNW9WTWhzZUtvcXVtQklwOTdIS2Y1cjZvZzMyNFg4Z0s3T3pXWTYwbVdJWGVCNnF3NHU4NGNEd05VbXE0cGxNMGpmOWktVDFhcm5iazM2VTd2YnJQOFpiN0NWcThvZkN4LXZobGs1dnlGWkc1NEdSUjBjNDhldVpWcnZndWtJYkRrSUx5Uk93NnRLY29yQTlSS1BMLVZnZVk5SFlBREhucUpwLUpIUzdwclhCcGpxeDhRMmNxYUk5SzRpTjQ1Q1Nf?oc=5\n"
     ]
    }
   ],
   "source": [
    "from google_news_feed import GoogleNewsFeed\n",
    "\n",
    "gnf = GoogleNewsFeed(language='en',country='US',resolve_internal_links=True, run_async=False)\n",
    "print(gnf.top_headlines()[0].link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a001bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.google.com/rss/articles/CBMi0AFBVV95cUxPaXhYSDhNMXExNW9WTWhzZUtvcXVtQklwOTdIS2Y1cjZvZzMyNFg4Z0s3T3pXWTYwbVdJWGVCNnF3NHU4NGNEd05VbXE0cGxNMGpmOWktVDFhcm5iazM2VTd2YnJQOFpiN0NWcThvZkN4LXZobGs1dnlGWkc1NEdSUjBjNDhldVpWcnZndWtJYkRrSUx5Uk93NnRLY29yQTlSS1BMLVZnZVk5SFlBREhucUpwLUpIUzdwclhCcGpxeDhRMmNxYUk5SzRpTjQ1Q1Nf?oc=5'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item = gnf.top_headlines()[0]\n",
    "item.link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e77765bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art = Article(item.link)\n",
    "art.download()\n",
    "art.parse()\n",
    "art.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fecbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def decode_google_news_url(source_url: str) -> str:\n",
    "    parsed = urlparse(source_url)\n",
    "    path_parts = parsed.path.strip(\"/\").split(\"/\")\n",
    "\n",
    "    if parsed.netloc == \"news.google.com\" and len(path_parts) >= 2 and path_parts[-2] == \"articles\":\n",
    "        b64 = path_parts[-1]\n",
    "        if not is_valid_base64(b64):\n",
    "            return source_url\n",
    "\n",
    "        try:\n",
    "            decoded = base64.b64decode(b64)\n",
    "            prefix = bytes([0x08, 0x13, 0x22])\n",
    "            suffix = bytes([0xd2, 0x01, 0x00])\n",
    "\n",
    "            if decoded.startswith(prefix):\n",
    "                decoded = decoded[len(prefix):]\n",
    "            if decoded.endswith(suffix):\n",
    "                decoded = decoded[:-len(suffix)]\n",
    "\n",
    "            length = decoded[0]\n",
    "            offset = 2 if length >= 0x80 else 1\n",
    "            actual = decoded[offset:offset + length].decode()\n",
    "\n",
    "            # If it's Google's internal redirect code, call their batch decode API\n",
    "            if actual.startswith(\"AU_yqL\"):\n",
    "                return fetch_decoded_batch_execute(b64)\n",
    "\n",
    "            return actual\n",
    "        except Exception as e:\n",
    "            print(f\"Decoding failed: {e}\")\n",
    "            return source_url\n",
    "    return source_url\n",
    "\n",
    "def is_valid_base64(s):\n",
    "    import re\n",
    "    return re.fullmatch(r'^[A-Za-z0-9+/=]+$', s) is not None\n",
    "\n",
    "def fetch_decoded_batch_execute(b64: str) -> str:\n",
    "    try:\n",
    "        rpc_payload = f\"\"\"[[[\"Fbv4je\",\"[\\\\\"garturlreq\\\\\",[[\\\\\"en-US\\\\\",\\\\\"US\\\\\",[\\\\\"FINANCE_TOP_INDICES\\\\\",\\\\\"WEB_TEST_1_0_0\\\\\"],null,null,1,1,\\\\\"US:en\\\\\",null,180,null,null,null,null,null,0,null,null,[1608992183,723341000]],\\\\\"en-US\\\\\",\\\\\"US\\\\\",1,[2,3,4,8],1,0,\\\\\"655000234\\\\\",0,0,null,0],\\\\\"{b64}\\\\\"]\",null,\"generic\"]]]\"\"\"\n",
    "\n",
    "        res = requests.post(\n",
    "            \"https://news.google.com/_/DotsSplashUi/data/batchexecute?rpcids=Fbv4je\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/x-www-form-urlencoded;charset=utf-8\",\n",
    "                \"Referrer\": \"https://news.google.com/\"\n",
    "            },\n",
    "            data={\"f.req\": rpc_payload},\n",
    "            timeout=10\n",
    "        )\n",
    "\n",
    "        s = res.text\n",
    "        header = '[\\\\\"garturlres\\\\\",\\\\\"'\n",
    "        footer = '\\\\\",'\n",
    "        if header not in s:\n",
    "            raise Exception(\"Header not found in response\")\n",
    "        start = s.split(header, 1)[1]\n",
    "        if footer not in start:\n",
    "            raise Exception(\"Footer not found\")\n",
    "        final_url = start.split(footer, 1)[0]\n",
    "        return final_url\n",
    "    except Exception as e:\n",
    "        print(f\"BatchExecute failed: {e}\")\n",
    "        return b64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61be846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding failed: Incorrect padding\n",
      "Resolved URL: https://news.google.com/rss/articles/CBMif0FVX3lxTFBkOVZURUF2c0dGOXFXV1JKNzVxbTBwdFZCTmthQkY3VnczX2s1M0dfTTJ6Q21PVlB1cEJNcDRoaWphNUlDYjhQczBuZXloYVFTYnVJcWgyVkNWM3I5LVVvY0NNZ184VHVyd0t3d1p3T0g1SzZfamc5XzZSSGhnWnM?oc=5\n"
     ]
    }
   ],
   "source": [
    "resolved = decode_google_news_url(google_link)\n",
    "print(\"Resolved URL:\", resolved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7412c0",
   "metadata": {},
   "source": [
    "Checking step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d03ad8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base64 String: CBMif0FVX3lxTFBkOVZURUF2c0dGOXFXV1JKNzVxbTBwdFZCTmthQkY3VnczX2s1M0dfTTJ6Q21PVlB1cEJNcDRoaWphNUlDYjhQczBuZXloYVFTYnVJcWgyVkNWM3I5LVVvY0NNZ184VHVyd0t3d1p3T0g1SzZfamc5XzZSSGhnWnM\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "google_link = [\n",
    "'https://news.google.com/rss/articles/CBMi7wJBVV95cUxNWWtnSWg5RWNoQ19UQUoyTDBzNFVHYUVfam9kUS1PTUNINkIyd01PaGgyckNFd1hkaHdqTHdzb0sxOVJWcWszM1hGWW9xOXlZWTAyZUhZdE5kWnQ0RnB3TmlBN3Nrc2daRjN0UmhKSjAtQmtCNndxYjBKYU9RVWM0NGw3WmhJdVA5Q2EwZWE3ZTJJU1JDdUlHbXVFeGlLVG1JTG9RRzgxdENSQ3o2WnBOV0lNUU50RFk1Y083UGxLNWowTzBoLXd1ZUUwVW9qbXJrQnRCVU00SVNDUnptcHNNMzFfWW5nd29ZV2lLdlB0U3NkUmJXNTFBSXNfTy1xbzQwQWRtN0ZrUHFsblpiaFc4VC1ZOVlQWktVNm9YMFdPUTZtMTNucDdLYXR3ajIwaWNKLUZQUHV5SEhILVFwNDIzSDlmMG1UY2dhTUhzcFBLTndGNjhweEVrUEhwSTlmSGtnVVlIUzlTOGZSS0t5SFJF?oc=5', #wsj.com\n",
    "'https://news.google.com/rss/articles/CBMif0FVX3lxTFBkOVZURUF2c0dGOXFXV1JKNzVxbTBwdFZCTmthQkY3VnczX2s1M0dfTTJ6Q21PVlB1cEJNcDRoaWphNUlDYjhQczBuZXloYVFTYnVJcWgyVkNWM3I5LVVvY0NNZ184VHVyd0t3d1p3T0g1SzZfamc5XzZSSGhnWnM?', #cnn.com\n",
    "][-1]\n",
    "\n",
    "\n",
    "parsed = urlparse(google_link)\n",
    "\n",
    "path_parts = parsed.path.strip(\"/\").split(\"/\")\n",
    "b64_string = path_parts[-1]\n",
    "\n",
    "print(\"Base64 String:\", b64_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f8bc6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Valid Base64: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_valid_base64(s):\n",
    "    return re.fullmatch(r'^[A-Za-z0-9+/=]+$', s) is not None\n",
    "\n",
    "print(\"Is Valid Base64:\", is_valid_base64(b64_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "199b68ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Bytes: b'\\x08\\x13\"\\x7fAU_yqLPd9VTEAvsGF9qWWRJ75qm0ptVBNkaBF7Vw3_k53G_M2zCmOVPupBMp4hija5ICb8Ps0neyhaQSbuIqh2VCV3r9-UocCMg_8TurwKwwZwOH5K6_jg9_6RHhgZs'\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "def fix_base64_padding(b64_string):\n",
    "    missing_padding = len(b64_string) % 4\n",
    "    if missing_padding:\n",
    "        b64_string += '=' * (4 - missing_padding)\n",
    "    return b64_string\n",
    "\n",
    "# Fix padding\n",
    "fixed_b64 = fix_base64_padding(b64_string)\n",
    "\n",
    "\n",
    "decoded = base64.b64decode(fixed_b64)\n",
    "print(\"Decoded Bytes:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b31b2d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_800\\4283351043.py:10: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
      "  for news in soup.findAll(\"item\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m news.title.text.strip(), html.url, s.text.strip(), \u001b[38;5;28mstr\u001b[39m(soup_content.select_one(\u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m).text)\n\u001b[32m     24\u001b[39m width = \u001b[32m80\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshorttxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup_page\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtextwrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtextwrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mget_items\u001b[39m\u001b[34m(soup)\u001b[39m\n\u001b[32m     12\u001b[39m a = s.select(\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m     13\u001b[39m a.extract()         \u001b[38;5;66;03m# extract lat 'See more on Google News..' link\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m html = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m soup_content = BeautifulSoup(html.text,\u001b[33m\"\u001b[39m\u001b[33mlxml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# perform basic sanitization:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:716\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    713\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_proxy(conn)\n\u001b[32m    715\u001b[39m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m httplib_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[32m    730\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:468\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    463\u001b[39m             httplib_response = conn.getresponse()\n\u001b[32m    464\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    465\u001b[39m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    466\u001b[39m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    467\u001b[39m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m             \u001b[43msix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m, in \u001b[36mraise_from\u001b[39m\u001b[34m(value, from_value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:463\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m         httplib_response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    465\u001b[39m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    466\u001b[39m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    467\u001b[39m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[32m    468\u001b[39m         six.raise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\http\\client.py:1378\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1380\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\http\\client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\http\\client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\ssl.py:1311\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1308\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1309\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1310\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "news_url=\"https://news.google.com/rss/search?q=technology\"\n",
    "rss_text=requests.get(news_url).text\n",
    "soup_page=BeautifulSoup(rss_text,\"xml\")\n",
    "\n",
    "def get_items(soup):\n",
    "    for news in soup.findAll(\"item\"):\n",
    "        s = BeautifulSoup(news.description.text, 'lxml')\n",
    "        a = s.select('a')[-1]\n",
    "        a.extract()         # extract lat 'See more on Google News..' link\n",
    "\n",
    "        html = requests.get(news.link.text)\n",
    "        soup_content = BeautifulSoup(html.text,\"lxml\")\n",
    "\n",
    "        # perform basic sanitization:\n",
    "        for t in soup_content.select('script, noscript, style, iframe, nav, footer, header'):\n",
    "            t.extract()\n",
    "\n",
    "        yield news.title.text.strip(), html.url, s.text.strip(), str(soup_content.select_one('body').text)\n",
    "\n",
    "width = 80\n",
    "for (title, url, shorttxt, content) in get_items(soup_page):\n",
    "    title = '\\n'.join(textwrap.wrap(title, width))\n",
    "    url = '\\n'.join(textwrap.wrap(url, width))\n",
    "    shorttxt = '\\n'.join(textwrap.wrap(shorttxt, width))\n",
    "    content = '\\n'.join(textwrap.wrap(textwrap.shorten(content, 1024), width))\n",
    "\n",
    "    print('-' * width)\n",
    "    print(content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a638260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Obtaining dependency information for webdriver-manager from https://files.pythonhosted.org/packages/b5/b5/3bd0b038d80950ec13e6a2c8d03ed8354867dc60064b172f2f4ffac8afbe/webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from webdriver-manager) (2.32.4)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl.metadata\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests->webdriver-manager) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests->webdriver-manager) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests->webdriver-manager) (2025.6.15)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.1.0 webdriver-manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Resolving Google News redirect...\n",
      "✅ Real URL: https://www.cnn.com/world/live-news/israel-iran-conflict-06-19-25-intl-hnk\n",
      "\n",
      "📰 Extracting article...\n",
      "Title: June 19, 2025 – Israel-Iran conflict\n",
      "Summary: White House press secretary Karoline Leavitt answered reporters’ questions on the Israel-Iran conflict and possible US involvement during a news briefing on Thursday.\n",
      "She read a statement from President Donald Trump saying he will decide whether to launch a US strike on Iran within the next two weeks.\n",
      "Here’s what else she said:On criticism from MAGA allies: Leavitt urged supporters to “trust in President Trump” after some MAGA allies criticized the administration’s consideration of increased US involvement in the conflict.\n",
      "“President Trump has incredible instincts, and President Trump kept America and the world safe in his first term as president in implementing a peace through strength foreign policy agenda,” Leavitt told reporters during Thursday’s press briefing.\n",
      "The president “has been very direct and clear: Iran can and should make a deal,” she added.\n",
      "Keywords: press, israeliran, iran, trump, weeks, leavitt, conflict, 19, 2025, president, weapon, told, nuclear, position\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from newspaper import Article\n",
    "import time\n",
    "\n",
    "\n",
    "def resolve_google_news_url(google_news_url: str) -> str:\n",
    "    \"\"\"Launch headless browser to resolve real article URL from Google News redirect.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Use new headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "    service = ChromeService(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(google_news_url)\n",
    "        time.sleep(2)  # wait for redirection\n",
    "        final_url = driver.current_url\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return final_url\n",
    "\n",
    "\n",
    "def extract_article_from_url(url: str) -> dict:\n",
    "    \"\"\"Use newspaper3k to extract full article content and metadata.\"\"\"\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()  # Extract summary & keywords\n",
    "\n",
    "    return {\n",
    "        \"title\": article.title,\n",
    "        \"text\": article.text,\n",
    "        \"summary\": article.summary,\n",
    "        \"keywords\": article.keywords,\n",
    "        \"top_image\": article.top_image,\n",
    "        \"authors\": article.authors,\n",
    "        \"published_date\": article.publish_date,\n",
    "        \"source_url\": url\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    gnews_url = \"https://news.google.com/articles/CBMiWG...\"  # Your redirect link\n",
    "    print(\"🔗 Resolving Google News redirect...\")\n",
    "    try:\n",
    "        real_url = resolve_google_news_url(google_link)\n",
    "        print(\"Real URL:\", real_url)\n",
    "\n",
    "        print(\"\\n📰 Extracting article...\")\n",
    "        article_data = extract_article_from_url(real_url)\n",
    "        print(\"Title:\", article_data['title'])\n",
    "        print(\"Summary:\", article_data['summary'])\n",
    "        print(\"Keywords:\", \", \".join(article_data['keywords']))\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92ee83f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White House press secretary Karoline Leavitt answered reporters’ questions on the Israel-Iran conflict and possible US involvement during a news briefing on Thursday.\\n\\nShe read a statement from President Donald Trump saying he will decide whether to launch a US strike on Iran within the next two weeks. He wants to allow diplomatic efforts to proceed before making a final decision on US military action, according to the statement. Leavitt also said a deal with Iran must include no enrichment of uranium.\\n\\nHere’s what else she said:\\n\\nOn criticism from MAGA allies: Leavitt urged supporters to “trust in President Trump” after some MAGA allies criticized the administration’s consideration of increased US involvement in the conflict.\\n\\n“President Trump has incredible instincts, and President Trump kept America and the world safe in his first term as president in implementing a peace through strength foreign policy agenda,” Leavitt told reporters during Thursday’s press briefing. “And with respect to Iran, nobody should be surprised by the president’s position that Iran absolutely cannot obtain a nuclear weapon — he’s been unequivocally clear about this for decades, not just as president, not just as a presidential candidate, but also as a private citizen.”\\n\\nOn Iran obtaining a nuclear weapon: Iran has “never been closer to obtaining a nuclear weapon,” she said, when pressed on Trump’s assessment that Iran is “a few weeks away” from it. Asked later to clarify whether that means Iran is close to starting to build a weapon or completing production of one, Leavitt said, “Iran has all that it needs to achieve a nuclear weapon.”\\n\\nOn Iran’s position: Tehran “is in a very weak and vulnerable position” after eight days of escalating conflict with Israel, the press secretary told CNN. The president “has been very direct and clear: Iran can and should make a deal,” she added.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4db6cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://newsapi.org/v2/top-headlines\"\n",
    "params = {'apiKey': 'bead73cb25824599aa56d874231b015d', 'country': 'us', 'q': 'AI', 'pageSize': 10, 'sortBy': 'publishedAt'}\n",
    "response = requests.get(url=url, params=params)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4eefd873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'totalResults': 14,\n",
       " 'articles': [{'source': {'id': None, 'name': 'New York Post'},\n",
       "   'author': 'Mitch Fink',\n",
       "   'title': 'Meet Mark Walter, the billionaire who agreed to buy Lakers in historic $10B deal - New York Post',\n",
       "   'description': 'After shelling out a record-breaking sum of approximately $10 billion, Mark Walter is soon to be the new owner of the Lakers.',\n",
       "   'url': 'https://nypost.com/2025/06/18/sports/meet-mark-walter-the-billionaire-who-agreed-to-buy-the-lakers/',\n",
       "   'urlToImage': 'https://nypost.com/wp-content/uploads/sites/2/2025/06/newspress-collage-7vjxg1pco-1750303605602.jpg?quality=75&strip=all&1750289275&w=1024',\n",
       "   'publishedAt': '2025-06-19T03:40:00Z',\n",
       "   'content': 'After shelling out a record-breaking sum of approximately $10 billion, Mark Walter is soon to be the new owner of the Lakers.\\r\\nThe 65-year-old Walter, a businessman with a net worth of $12.5 billion,… [+2661 chars]'},\n",
       "  {'source': {'id': None, 'name': 'The Spun'},\n",
       "   'author': 'Andrew Holleran',\n",
       "   'title': \"There's Growing Push For Caitlin Clark To Leave The WNBA - Yahoo Sports\",\n",
       "   'description': 'There\\'s a growing push for Caitlin Clark to \"leave\" the WNBA following Tuesday night\\'s incident. Clark, the No. 1 overall pick in the 2024 WNBA Draft, was...',\n",
       "   'url': 'https://thespun.com/wnba/theres-growing-push-for-caitlin-clark-to-leave-the-wnba',\n",
       "   'urlToImage': 'https://media.zenfs.com/en/the_spun_articles_751/bf4fd19b1e88153ebc5cecfb1ed6814c',\n",
       "   'publishedAt': '2025-06-19T01:36:12Z',\n",
       "   'content': 'There\\'s Growing Push For Caitlin Clark To Leave The WNBA originally appeared on The Spun.\\r\\nThere\\'s a growing push for Caitlin Clark to \"leave\" the WNBA following Tuesday night\\'s incident.\\r\\nClark, the… [+2651 chars]'},\n",
       "  {'source': {'id': None, 'name': 'Boston Herald'},\n",
       "   'author': 'Flint McColgan',\n",
       "   'title': 'Not Guilty: Jury acquits Karen Read on murder, manslaughter charges, guilty on OUI - Boston Herald',\n",
       "   'description': 'Karen Read wept and the crowd outside burst into cheers after a jury cleared the 45-year-old of every count against her except for one — drunken driving — in a grueling retrial for the …',\n",
       "   'url': 'https://www.bostonherald.com/2025/06/18/karen-read-found-not-guilty-on-murder-manslaughter-guilty-on-oui/',\n",
       "   'urlToImage': 'https://www.bostonherald.com/wp-content/uploads/2025/06/AP25169728202659.jpg?w=1024&h=739',\n",
       "   'publishedAt': '2025-06-19T00:45:00Z',\n",
       "   'content': 'Karen Read wept and the crowd outside burst into cheers after a jury cleared the 45-year-old of every count against her except for one — drunken driving — in a grueling retrial for the death of her B… [+4845 chars]'},\n",
       "  {'source': {'id': None, 'name': 'Variety'},\n",
       "   'author': 'Gene Maddaus',\n",
       "   'title': 'Justin Baldoni Can Seek Messages Between Taylor Swift and Blake Lively, Judge Rules - Variety',\n",
       "   'description': 'Justin Baldoni can obtain messages between Taylor Swift and Blake Lively, a judge ruled Wednesday.',\n",
       "   'url': 'https://variety.com/2025/film/news/justin-baldoni-seek-taylor-swift-blake-lively-judge-rules-1236435847/',\n",
       "   'urlToImage': 'https://variety.com/wp-content/uploads/2025/01/swift-baldoni-lively-split.jpg?w=1000&h=563&crop=1',\n",
       "   'publishedAt': '2025-06-19T00:05:00Z',\n",
       "   'content': 'A judge ruled Wednesday that Justin Baldoni can obtain messages between Blake Lively and Taylor Swift that pertain to “It Ends With Us” and the legal melee between the film’s co-stars.\\r\\nLively had as… [+1636 chars]'},\n",
       "  {'source': {'id': None, 'name': 'BaltimoreRavens.com'},\n",
       "   'author': None,\n",
       "   'title': 'Marlon Humphrey, Kyle Hamilton React to Ravens Adding Jaire Alexander | News & Notes - Baltimore Ravens',\n",
       "   'description': 'The Ravens have a plan to help Jaire Alexander avoid injuries. John Harbaugh gives an update on Emery Jones Jr. Baltimore’s dates for joint practices with the Colts and Commanders have been announced.',\n",
       "   'url': 'https://www.baltimoreravens.com/news/marlon-humphrey-kyle-hamilton-john-harbaugh-reaction-jaire-alexander',\n",
       "   'urlToImage': 'https://static.clubs.nfl.com/image/upload/t_editorial_landscape_12_desktop/ravens/tvjqdg3mdwnnr02sfjm6',\n",
       "   'publishedAt': '2025-06-18T22:50:40Z',\n",
       "   'content': None},\n",
       "  {'source': {'id': None, 'name': 'Hollywood Reporter'},\n",
       "   'author': 'Aaron Couch',\n",
       "   'title': 'Danny Boyle Talks ‘28 Years Later’ Trilogy Plan, Turning Down ‘Alien’ and the Time ‘127 Hours’ Made Pixar Animators Faint - The Hollywood Reporter',\n",
       "   'description': 'In a career-spanning interview, the Oscar-winning auteur also reflects on Ewan McGregor shaving his \"incredible hair\" before being cast in \\'Trainspotting,\\' Cillian Murphy landing \\'28 Days Later\\' thanks to a single sentence, and why Bond isn\\'t in his future: \"…',\n",
       "   'url': 'http://www.hollywoodreporter.com/movies/movie-features/danny-boyle-talks-28-years-later-trilogy-plan-1236293456/',\n",
       "   'urlToImage': 'https://www.hollywoodreporter.com/wp-content/uploads/2025/06/DF-08687-e1750190889360.jpg?w=1440&h=810&crop=1',\n",
       "   'publishedAt': '2025-06-18T21:46:04Z',\n",
       "   'content': 'On Danny Boyle’s 11th birthday, he and his twin sister were given a special treat, when their parents took them to the movies for the first time. Boyle and his father saw the Henry Fonda war epic Bat… [+14327 chars]'},\n",
       "  {'source': {'id': None, 'name': 'Live Science'},\n",
       "   'author': 'Jane Palmer',\n",
       "   'title': 'Just 1 dose of magic mushroom compound eases depression for at least 5 years in most patients, small study suggests - Live Science',\n",
       "   'description': 'Half a decade after receiving a psychedelic treatment for depression, two-thirds of patients in a new study remained in remission.',\n",
       "   'url': 'https://www.livescience.com/health/mind/one-psychedelic-psilocybin-dose-eases-depression-for-years-study-reveals',\n",
       "   'urlToImage': 'https://cdn.mos.cms.futurecdn.net/zpjpV9CguFXkFSbyCNDf5n.jpg',\n",
       "   'publishedAt': '2025-06-18T21:31:45Z',\n",
       "   'content': 'DENVER—Psilocybin\\r\\n, the main psychoactive ingredient in magic mushrooms, can alleviate depression for at least five years after a single dose, a new study finds.\\r\\nThe research, presented June 18 at … [+4959 chars]'},\n",
       "  {'source': {'id': 'associated-press', 'name': 'Associated Press'},\n",
       "   'author': None,\n",
       "   'title': 'Stanley Cup damaged as the Florida Panthers celebrate a second straight championship - AP News',\n",
       "   'description': 'The Stanley Cup is a little banged up, thanks to the Florida Panthers’ celebration of back-to-back titles. The bowl of the famous trophy is cracked and the bottom is dented. The Panthers won their second consecutive championship on home ice Tuesday night, bea…',\n",
       "   'url': 'https://apnews.com/article/panthers-stanley-cup-crack-60bd5610c41f16885a09ade63557f880',\n",
       "   'urlToImage': 'https://dims.apnews.com/dims4/default/7ef351a/2147483647/strip/true/crop/3251x1829+0+170/resize/1440x810!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2Fc5%2F76%2F4e171dcdeb73b799c3c601c387ce%2F13044ae9fb2c44cc97c553c9c3f5d90f',\n",
       "   'publishedAt': '2025-06-18T21:25:00Z',\n",
       "   'content': 'FORT LAUDERDALE, Fla. (AP) The Stanley Cup is a little banged up, thanks to the Florida Panthers celebration of back-to-back titles.\\r\\nThe bowl of the famous trophy is cracked and the bottom is dented… [+1078 chars]'}]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a878d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\"))\n",
      "4 requests to https://twitter.com/search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click failed, giving up.\n",
      "Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\"))\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mScraperException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m max_results = \u001b[32m10\u001b[39m\n\u001b[32m      5\u001b[39m tweets = []\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msntwitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTwitterSearchScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:1763\u001b[39m, in \u001b[36mTwitterSearchScraper.get_items\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1760\u001b[39m params = {\u001b[33m'\u001b[39m\u001b[33mvariables\u001b[39m\u001b[33m'\u001b[39m: variables, \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: features}\n\u001b[32m   1761\u001b[39m paginationParams = {\u001b[33m'\u001b[39m\u001b[33mvariables\u001b[39m\u001b[33m'\u001b[39m: paginationVariables, \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: features}\n\u001b[32m-> \u001b[39m\u001b[32m1763\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_TwitterAPIType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGRAPHQL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaginationParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_by_raw_query\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_timeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graphql_timeline_instructions_to_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_by_raw_query\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_timeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:915\u001b[39m, in \u001b[36m_TwitterAPIScraper._iter_api_data\u001b[39m\u001b[34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    914\u001b[39m \t_logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m \tobj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[32m    918\u001b[39m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:883\u001b[39m, in \u001b[36m_TwitterAPIScraper._get_api_data\u001b[39m\u001b[34m(self, endpoint, apiType, params, instructionsPath)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_api_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint, apiType, params, instructionsPath = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m \t\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_guest_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    884\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType.GRAPHQL:\n\u001b[32m    885\u001b[39m \t\tparams = urllib.parse.urlencode({k: json.dumps(v, separators = (\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.items()}, quote_via = urllib.parse.quote)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:825\u001b[39m, in \u001b[36m_TwitterAPIScraper._ensure_guest_token\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._guestTokenManager.token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    824\u001b[39m \t_logger.info(\u001b[33m'\u001b[39m\u001b[33mRetrieving guest token\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \tr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_baseUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_guest_token_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m (match := re.search(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.cookie = decodeURIComponent\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgt=(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+); Max-Age=10800; Domain=\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.twitter\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.com; Path=/; Secure\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m);\u001b[39m\u001b[33m'\u001b[39m, r.text)):\n\u001b[32m    827\u001b[39m \t\t_logger.debug(\u001b[33m'\u001b[39m\u001b[33mFound guest token in HTML\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\snscrape\\base.py:275\u001b[39m, in \u001b[36mScraper._get\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\Repos\\trend-analyzer-twitter-bot\\venv\\Lib\\site-packages\\snscrape\\base.py:271\u001b[39m, in \u001b[36mScraper._request\u001b[39m\u001b[34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[39m\n\u001b[32m    269\u001b[39m \t_logger.fatal(msg)\n\u001b[32m    270\u001b[39m \t_logger.fatal(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mReached unreachable code\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mScraperException\u001b[39m: 4 requests to https://twitter.com/search?f=live&lang=en&q=AI&src=spelling_expansion_revert_click failed, giving up."
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "query = \"AI\"\n",
    "max_results = 10\n",
    "tweets = []\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "    if i >= max_results:\n",
    "        break\n",
    "    tweets.append({\n",
    "        \"content\": tweet.content,\n",
    "        \"url\": tweet.url,\n",
    "        \"username\": tweet.user.username,\n",
    "        \"created\": tweet.date.isoformat()\n",
    "    })\n",
    "\n",
    "for t in tweets:\n",
    "    print(f\"🕊 {t['username']} said: {t['content'][:80]}...\\n🔗 {t['url']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f5c41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snscrape\n",
      "  Obtaining dependency information for snscrape from https://files.pythonhosted.org/packages/82/90/82b86fdf8e3a3faef16036b57764e969a931285cce1d82956ccda60daa2c/snscrape-0.7.0.20230622-py3-none-any.whl.metadata\n",
      "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from snscrape) (2.32.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from snscrape) (5.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from snscrape) (4.13.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from snscrape) (3.18.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from beautifulsoup4->snscrape) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from beautifulsoup4->snscrape) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests[socks]->snscrape) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests[socks]->snscrape) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests[socks]->snscrape) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests[socks]->snscrape) (2025.6.15)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\admin\\onedrive\\documents\\repos\\trend-analyzer-twitter-bot\\venv\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Downloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/74.8 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/74.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 20.5/74.8 kB 330.3 kB/s eta 0:00:01\n",
      "   --------------------- ------------------ 41.0/74.8 kB 393.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 74.8/74.8 kB 687.0 kB/s eta 0:00:00\n",
      "Installing collected packages: snscrape\n",
      "Successfully installed snscrape-0.7.0.20230622\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0aee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
